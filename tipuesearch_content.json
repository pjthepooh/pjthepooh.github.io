{"pages":[{"url":"http://www.pjthepooh.com/pages/../about.html","text":"My name is PJ, born and raised in Guangzhou, China . I'm currently a data scientist in Silicon Valley , immersing myself in a world of science and new technology. I truly believe that most great decisions are driven by data. I'm willing to learn as much as I can to become a full-stack data scientist. I love sports such as swimming, basketball, snowboarding and bicycling. To be a better version of myself, I love to travel! My another big interest is teaching. I believe that sharing knowledge takes a key role in human society. As a first-generation immigrant in U.S., I would love to contribute my little effort to help the Chinese immigrant community. I speak Cantonese, Mandarin and English.","tags":"Life","loc":"http://www.pjthepooh.com/pages/../about.html","title":"About"},{"url":"http://www.pjthepooh.com/ransac_in_r.html","text":"Intuition Random sample consensus (RANSAC) is one of the techniques that estimate model parameters while the data contains outliers. This technique is not very famous in statistical field, but has been widely used in computer vision. But why? This is also something I want to discuss in this post. A year ago I was working on a project written in R, and wanted to use this technique; however, I was not able to find a package or immediate code in R, but in many other languages such as Matlab, Python and etc. RANSAC's idea is intuitive and the implementation is simple, so I decided to code it up, but I had not actually worked on it until now. In [1]: suppressWarnings ( suppressPackageStartupMessages ({ library ( gridExtra ) library ( pryr ) library ( ggplot2 ) })) First, we need to generate a dataset with trend and noise components, where the trend is 'clean' data points with strong signal and the noise is something that will potentially damage the signal. The noise data is uniformly distributed in a square grid as you can see below. 9 datasets are generated with different noise-to-clean ratios. In [2]: gen_dummy_data <- function ( x , y , noise_ratio = 0.5 ) { if ( length ( x ) != length ( y )) { stop ( \"Length of x and y have to be the same.\" ) } n <- length ( x ) trend <- data.frame ( x = x , y = y ) noise <- data.frame ( x = runif ( noise_ratio * n , min ( x ), max ( x )), y = runif ( noise_ratio * n , min ( y ), max ( y ))) df <- rbind ( trend , noise ) df <- df [ sample ( 1 : nrow ( df ), nrow ( df )), , drop = FALSE ] return ( df ) } In [3]: seed = 1110 m = 100 x <- 1 : m y <- x + rnorm ( m , 0 , 3 ) noise_ratio = c ( 0.25 , 0.5 , 1 , 2 , 3 , 4 , 5 , 8 , 10 ) set.seed ( seed ) df_list <- list () do.call ( partial ( grid.arrange , nrow = 3 ), lapply ( noise_ratio , function ( r ) { df <- gen_dummy_data ( x , y , noise_ratio = r ) df_list [[ as.character ( r )]] <<- df ggplot () + geom_point ( data = df , aes ( x , y )) + ggtitle ( sprintf ( \"noise_ratio = %s\" , r )) }) ) As noise ratio increases, it becomes harder to notice the trend. Let's see if RANSAC is able to detect the trend from the noisy background. Code The R code below is my own version of RANSAC which does not change its principle. This is the first draft of the code for prototype. Please review some materials (or at least Wikipedia ) about this technique for more details. Note: MAE is used for goodness of fit of a model The probablity that the algorithm selects only inliers in some iteration is set to be 0.99 to solve for the maximum number of iteration, k. The initial threshold value is set to be the median of absolute errors of a model fitted using all data points. Default noise ratio is 0.7 to derive other parameters if they are not provided. In [4]: #' Random sample consensus (RANSAC) #' #' @param data dataframe with target column and model matrix columns #' @param y_col name of target column #' @param model model function to fit #' @param model_args list, arguments to the model #' @param n the minimum number of data values required to fit the model #' @param k the maximum number of iterations allowed in the algorithm #' @param t a threshold value for determining when a data point fits a model #' @param d the number of close data values required to assert that a model fits well to data #' #' @return a list of output contains best fitted model, inliers, outliers #' RANSAC <- function ( data , y_col , model , model_args , n = 2 , k = NA , t = NA , d = NA , verbose = FALSE ) { default_ratio = 0.7 model_args_all <- c ( model_args , data = list ( data )) model_all <- do.call ( model , args = model_args_all ) err_abs <- abs ( data [, y_col ] - predict ( model_all , newdata = data [, names ( df ) != \"y\" , drop = FALSE ])) if ( is.na ( t )) { t <- quantile ( err_abs , 0.5 , names = FALSE ) } if ( is.na ( d )) { d <- nrow ( data ) * default_ratio } if ( is.na ( k )) { k <- as.integer ( log ( 1 - 0.99 ) / log ( 1 - default_ratio &#94; n )) } mae_best <- mean ( err_abs ) ind <- 1 : nrow ( data ) inliers <- c () i = 1 if ( verbose ) { progress_pct = round ( quantile ( seq ( 1 , k , length.out = 10 ), probs = seq ( 0.1 , 1 , length.out = 10 )), 0 ) cat ( sprintf ( \"Begin RANSAC algoritm with parameters:\\nn = %s\\nk = %s\\nt = %s \\nd = %s\\n\\n\" , n , k , t , d )) } while ( i <= k ) { if ( verbose && ( i %in% progress_pct ) ) { cat ( sprintf ( \"Completion percentage: %s\\n\" , names ( progress_pct )[ which ( i == progress_pct )])) } inliers_case <- sample ( ind , n ) model_args_case <- c ( model_args , data = list ( data [ inliers_case , , drop = FALSE ])) model_case <- do.call ( model , model_args_case ) predict_args_case = list ( object = model_case , newdata = data [ - inliers_case , , drop = FALSE ] ) yhat <- do.call ( predict , predict_args_case ) y <- data [ - inliers_case , y_col ] res <- abs ( yhat - y ) inliers_case <- c ( inliers_case , names ( res )[ which ( res < t )]) if ( length ( inliers_case ) > d ) { data_inliers <- data [ inliers_case , , drop = FALSE ] model_inliers_args <- c ( model_args , data = list ( data_inliers )) model_inliers <- do.call ( model , model_inliers_args ) predict_inliers_args_case = list ( object = model_inliers , newdata = data_inliers ) mae_inliers <- mean ( abs ( data_inliers [, y_col ] - do.call ( predict , predict_inliers_args_case ))) if ( mae_inliers < mae_best ) { model_best <- model_inliers mae_best <- mae_inliers inliers <- inliers_case } } i = i + 1 } if ( ! exists ( \"model_best\" )) { warning ( \"Final model could not be found, using all data to fit the model. \" ) model_best <- model_all } else { model_best [ \"call\" ] <- sprintf ( \"Model with formula with %s\" , model_args [[ \"formula\" ]]) } inliers_df = data [ sort ( inliers ), , drop = FALSE ] outliers_df = data [ setdiff ( ind , sort ( inliers )), , drop = FALSE ] if ( verbose ) { cat ( sprintf ( \"%s (%.1f%%) inliers out of %s total data points have been used. \\n\" , length ( inliers ), length ( inliers ) / nrow ( data ) * 100 , nrow ( data ))) } return ( list ( model = model_best , inliers = inliers_df , outliers = outliers_df )) } Next we just loop through different scenarios. For convenience, I hard coded n=5 and k=500 here, which are supposed to be further optimized. In [5]: plot_list <- list () for ( r in noise_ratio ) { args_list = list ( data = df_list [[ as.character ( r )]], y_col = \"y\" , model = lm , model_args = list ( formula = \"y ~ x\" ), n = 5 , k = 500 , t = NA , d = m / ( 1 + r ), verbose = FALSE ) set.seed ( seed ) fit <- RANSAC ( data = args_list [[ \"data\" ]], y_col = args_list [[ \"y_col\" ]], model = args_list [[ \"model\" ]], model_args = args_list [[ \"model_args\" ]], n = args_list [[ \"n\" ]], k = args_list [[ \"k\" ]], t = args_list [[ \"t\" ]], d = args_list [[ \"d\" ]], verbose = args_list [[ \"verbose\" ]] ) p <- ggplot () + geom_point ( data = args_list [[ \"data\" ]], aes ( x = x , y = y ), size = 0.6 ) + stat_smooth ( data = args_list [[ \"data\" ]], aes ( x = x , y = y ), method = \"lm\" , alpha = 0 , color = \"blue\" , size = 1.2 ) + geom_point ( data = fit [[ \"inliers\" ]], aes ( x , y ), color = \"red\" , size = 0.6 ) + stat_smooth ( data = fit [[ \"inliers\" ]], aes ( x = x , y = y ), method = \"lm\" , alpha = 0 , color = \"green\" , size = 1.2 ) + ggtitle ( sprintf ( \"noise_ratio = %s\" , r )) plot_list [[ as.character ( r )]] <- p } In [6]: do.call ( partial ( grid.arrange , nrow = 3 ), lapply ( plot_list , function ( x ) { x })) The blue lines are regular OLS regressions using all data points, so of course they do not fit the trend well. The red points are the inliers selected by the algorithm, and the green lines are best fits. As you can see, RANSAC is able to detect the trend even with high noise rate. It only fails on the last one, where noise-to-clean ratio is 10:1. If you further optimize the parameters, the algorithm may not fail. Discussion Going back to the question: why is RANSAC not widely used in statistics field? There are strong reasons why it is being used in computer vision. There is a concept of noise in CV, where image noise should not give any information to an image. On ther other hand, RANSAC discards a large portion of data, making no attempt to accommodate the outliers. In simple word, it treats the detected outliers that genuinely don't belong to the sample, which is somehow against the philosophy of statistics. Not many statistical approaches attempt to do so. Also, there are many other well-performing robust estimation techniques that better obey statistical philosophy. Ending RANSAC is a quick, simple, intuitive approach for many statistical analyses such as anomaly detection, forecasting, pattern recognition and etc. As people who love statistics and data, we should put it in our tool set. Hope this is helpful and please let me know of any questions regarding this post.","tags":"Data Science","loc":"http://www.pjthepooh.com/ransac_in_r.html","title":"RANSAC in R"},{"url":"http://www.pjthepooh.com/how-to-make-a-blog-with-pelican.html","text":"Intuition There are many benefits of creating a personal website or blog, though it takes time to maintain. Once you decide to make one, finding a framework that you feel comfortable with is important. This website is generated by Pelican , a static site generator powered by python. In this post, I will demonstrate step by step on how to make a blog using Pelican. There are other nice frameworks out there, e.g. Jekyll (ruby), Hexo (javascript) and Nikola (python). These frameworks work well on Windows, OSX, and Linux. For those who are interested in Hexo, please check out my friend's post about Hexo with a step-by-step guide. Ok. Let's get started with Pelican. Step-by-Step Guide Prerequisites I assume you are already comfortable working with python and git . Although it is optional, I recommend the project is built under a virtual environment . Additionally to python and git , which I assume you already installed, you need to install pelican , markdown (as I'm going to use markdown to write contents). I'm going to work on the directory ~/pelican_demo/ for this demo: mkdir ~/pelican_demo cd ~/pelican_demo We next generate a virtual environment and then build necessary libraries within it. virtualenv env source env/bin/activate pip install pelican markdown You may need to install other tools along the way, but let's keep it basic first. Once above packages are installed successfully, we can move to the next step. Quick Start Pelican provides a quick start function, pelican-quickstart , that generates a framework skeleton. I suggest not to create the project in the same directory as where the virtual environment is in; instead, create another directory, e.g. src , for example. mkdir src cd src Once you run the command pelican-quickstart , there is a series of questions popped up, and just answer it one by one. For URL prefix, you can choose n for now. You can leave most of the answers as default (the capitalized one). $ pelican-quickstart Welcome to pelican-quickstart v3.6.3. This script will help you create a new Pelican-based website. Please answer the following questions so this script can generate the files needed by Pelican. > Where do you want to create your new web site? [ . ] > What will be the title of this web site? Pelican Demo > Who will be the author of this web site? PJ > What will be the default language of this web site? [ en ] > Do you want to specify a URL prefix? e.g., http://example.com ( Y/n ) n > Do you want to enable article pagination? ( Y/n ) > How many articles per page do you want? [ 10 ] 8 > What is your time zone? [ Europe/Paris ] America/Los_Angeles > Do you want to generate a Fabfile/Makefile to automate generation and publishing? ( Y/n ) > Do you want an auto-reload & simpleHTTP script to assist with theme and site development? ( Y/n ) > Do you want to upload your website using FTP? ( y/N ) > Do you want to upload your website using SSH? ( y/N ) > Do you want to upload your website using Dropbox? ( y/N ) > Do you want to upload your website using S3? ( y/N ) > Do you want to upload your website using Rackspace Cloud Files? ( y/N ) > Do you want to upload your website using GitHub Pages? ( y/N ) Done. Your new project is available at /Users/pjhuang/pelican_demo/src Now you should have the following folder structure in your src directory. $ tree . ├── Makefile ├── content ├── develop_server.sh ├── fabfile.py ├── output ├── pelicanconf.py └── publishconf.py 2 directories, 5 files Here I will briefly explain these files and folders. Makefile , a program building tool (no need to touch it) content , the directory that will store all your content, e.g. posts, images, pages and etc. Most of your writing will be stored here. develop_server.sh a shell script that boosts your develop server. (again, no need to touch it) fabfile.py You can define your own functions in this file to make the deployment process easier by using fabric. For more details please review Fabric doc . The usage of fab is optional. output , a directory that contains all your outputs that will be shown in the website, e.g. HTMLs, CSSs, images and etc. The website will only consume the files inside this folder. pelicanconf.py a configuration file. You will customize most of your settings in this file. As there are many things you can change, please spend some time to review the setting doc . publishconf.py a configuration file that will be used when publishing. By default, this script will import pelicanconf.py , so you can image this file contains some extra settings when the website goes online. Writing Content Posts and pages are written in Markdown (.md), reStructuredText (.rst) or HTML (.html). I personally prefer Markdown. Pelican will take your plain text file along with the HTML, CSS and whatever necessary files in the some other locations such as your theme directory (will be discussed later) to compile into HTMLs, outputting files into output directory. Let's write a dummy post. New Post To be more organized we can create a subdirectory to store the post. For example, I create a folder named 2016 to store all the posts written in the year 2016. Open your favourite text editor and type in the following and save it as a markdown (*.md) file in directory src/content/2016/ mkdir content/2016 vim content/2016/my_first_post.md Title : My First Post Date : 2016 - 09 - 30 23 : 40 Modified : 2016 - 09 - 30 23 : 40 Category : Pelican Tags : pelican , publishing , python Slug : my - first - post Authors : PJ Summary : This is my first post ! The tags above are called metadata. The necessary fields are Title and Date , others are optional but recommended. New Pages Writing a new page is pretty much the same as writing a post. The main difference is your page files need to be within a folder named pages under content . For example, we can create a About page like this. mkdir content/pages vim content/pages/about.md Title: About Date: 2016-09-30 23:45 Modified: 2016-09-30 23:45 Category: Pelican Tags: pelican, about Slug: ../about Authors: PJ <p> My name is PJ, born and raised in <a href= \"https://en.wikipedia.org/wiki/Guangzhou\" > Guangzhou, China </a> . </p> Now you should have a folder structure like this. src ├── Makefile ├── content │ ├── 2016 │ │ └── my_first_post.md │ └── pages │ └── about.md ├── develop_server.sh ├── fabfile.py ├── output ├── pelicanconf.py ├── pelicanconf.pyc └── publishconf.py Make Files Now you may want to take a look of the actual website content and layout. You can do that by using the built-in make functions defined in the Makefiles and develop_server.sh . Although you don't have to use all make functions to maintain the blog, there are several handy ones you may use frequently. make html - (re)generate the web site make clean - remove the generated files make devserver - start local server (port 8000 as default) make stopserver - stop local server make publish - generate using production First, we need to generate website files, i.e. HTML. $ make html pelican /Users/pjhuang/pelican_demo/src/content -o /Users/pjhuang/pelican_demo/src/output -s /Users/pjhuang/pelican_demo/src/pelicanconf.py Done: Processed 1 article, 0 drafts, 1 page and 0 hidden pages in 0.14 seconds. Second, we start the local server so we can view it locally. make devserver Once the server starts, you can open your browser and type in localhost:8000 , and you will see your website. When the local server is running, any changes in content files will be detected and output files will be regenerated automatically. You can refresh the browser to update the website. Before you publish it (see later section), run this: make publish You can stop the server through: make stopserver Themes This is cool right? What even cooler for Pelican is that it provides many beautiful themes from its themes repository . You can view a screenshot of all these themes in this site . By just a couple steps, you can change your themes quickly. For example, if you want to use the theme blue-penguin , below is what you need to do: Clone this theme's repo in your src directory (you can clone it anywhere but you need to change the theme path in pelicanconf.py accordingly). git clone https://github.com/jody-frankowski/blue-penguin.git And then change (add if you don't see it) the variable THEME in pelicanconf.py . That's it! THEME = 'blue-penguin' You can create your own theme using Jinja . There is a great document about theme creation in Pelican. Plugins Another cool thing about Pelican is it provides a lot of plugins that satisfy most of your needs. Their repo usually have well documented instruction, so here I'm not going to show examples. The steps are similar to those of setting up themes. Again, there is a good documentation about plugin creation if you are interested in. Deployment Hosting on Github Pages Github Pages is one of the best places to host a static website. It is free and provides a 300Mb storage space, which is not bad for a static blog. The deployment process is simple. Register a Github account using your customized username, say, statslover. Create a new repository with the name as this format, <your_username>.github.io and set it as public. In my example, the repo name will be statslover.github.io. Push only your output directory to this repo. There are several ways to finish step 3. As only files inside output directory will be used, so one can initiate a git rooted there and push whatever inside to the repo; however, this is not practical at all because the files including your .git/ could be deleted if you run make clean . Option I You may create a folder that will sync up your output/ directory. For example, I generated a directory named github_page/ one level above src , and wrote a bash script that makes it synced with output/ . My git is in github_page/ and every time before pushing to origin, I sync two folders by running the script. This avoids my git repo being deleted or modified. The command can be something like that: rsync -avz --delete --exclude '.git/' output/ ../github_page Just push it to master as you would do in usual after rsync. cd ~/pelican_demo/github_page git init git remote add origin git@github.com:<your_username>/<your_username>.github.io.git git add . git commit -m\"init commit\" git push -u origin master Option II Another way is to use ghp-import . This module will package your output and push to the repo. I don't like this approach as it generates a gh-pages branch that is the site's document root, which means I would have to deal with two branches somehow. Here are the steps: cd ~/pelican_demo/src pip install ghp-import ghp-import output git push git@github.com:<your_username>/<your_username>.github.io.git gh-pages:master There are many other ways. The process is good as long as you think it is smooth. Once this is done, your website will be shown in the URL <your_username>.github.io. Hosting on other servers If you want to use another server to host the website, you can just simply copy (scp) the files from output/ to the appropriate location of your server. More easily, you can just in that location git clone or pull from your Github repo. Although this requires one more step, but you gain the benefits of using version control. For example, my current workflow is like writting content locally >> generate output files >> sync folder >> push to Github >> pull repo from Github to AWS . This is a bit inconvenient but the work is minimal, so I'm fine with it. Ending Now you should be able to make a static website using Pelican. Hope you think this is helpful. If you have any advice or questions, please leave comments to let me know.","tags":"Geek","loc":"http://www.pjthepooh.com/how-to-make-a-blog-with-pelican.html","title":"How to Make a Blog with Pelican"},{"url":"http://www.pjthepooh.com/i-chose-pelican.html","text":"I have been thinking about making a personal website for awhile, but just could not find time to actually work on it. It takes time but I know it is worth it. A month ago, I was struggling on what framework to use and where to host it. Until recently, I learned and tried three different frameworks and finally decided what is the best for me. Static or Dynamic This was the first question I asked myself. I was biased to dynamic because I want my website to connect with a database, though it is not necessary. The process of setting up the whole dynamic website would definitely help me explore a new area in my life. There is a short way and a long way to accomplish it. The short way is to use a web-hosting service such as Wordpress, Blogger and Weebly; while the long is to build it myself using some web development platform. Anyway, I chose static at the end. Below are the reasons. Django One of the main purposes I'm making a website is trying to learn new things through the process, so of course the drag-and-click feature is not something I would look for. At first I started to learn django. After two weeks of researching online materials, I was able to make a decent skelton successfully connecting to a database, but I was exhausted and not satisfied. I think django is an overkill for my website. Although django is great for professional web development, the investment of learning this framework does not give me a quick pay back. The learning process did help me build a good understanding of web development; I gave up though. Wordpress Using a hosting server, users are able to SSH the server but do not have permission to install applications. Plus, it is costly compared to other approaches. I decided to give it a try and installed wordpress in my AWS EC2 micro. That way at least I would have full control and be able to play around with MySQL. Things went pretty well in the first couple days, but my server began to suffer running out of RAM soon after. In the following week, I kept trying many different ways to optimize the configuration of the web, PHP, and database, nothing seemed to work. I finally moved the database to an RDS instance, but my website was still not able to be hosted smoothly, loading very slowly and even timing out. As it was too annoying, I decided to move to a static web. Pelican For static website, there are several popular platforms, e.g. Hexo (javascript), Jekyll (ruby) and Pelican (python). Users only need to know some basic syntax of the language to begin. I picked pelican simply because it is written in python. All of these frameworks have a large community of developers and users, so there are tons of themes and plugins available. The initial setup of Pelican is simple and it will provide a dummy template to begin with; however, I did need to spend some time on customization and settings. With Pelican's simplicity, I can focus on my writing and not being distracted by thinking how to format my post because the theme does all the jobs for me. If I want to edit the theme, it is easy enough too. So far from writing to deploying a new post, I do not need to touch the mouse, which is a pretty smooth process. Ending Last month was all about trial and error. In the end I was able to learn new things. Most importantly I built my website the way I like. Of course there are some downsides of using Pelican, but I can bear them. I may write up a post about how to build a blog with Pelican later :)","tags":"Geek","loc":"http://www.pjthepooh.com/i-chose-pelican.html","title":"I Chose Pelican"},{"url":"http://www.pjthepooh.com/fall-off-the-bike.html","text":"Today is a bad day. I fell off my bike (bicycle, not motorcycle), and it was my first time falling off. As I did not have any protection, except my helmet of course, and it was in a pretty damn fast speed, so my legs and knees were bruised. Most sadly, my bike crashed on the ground and the handler was deformed. Yup, I bought a new bike a month ago and was about to ride it to work on a daily basis. This morning I was trying to get familiar with the route from home to the office. It was Market St. There was a bus stoping in front of me blocking half of the road. I wanted to accelerate and pass it on the left (yes right? You know you won't stop if you are on a two-wheeled machine). How the hell I could not know there are muni tracks on the middle of Market St (yes right? I must have been living in the city for too long to notice it). I should have noticed it, or I should have never passed buses like that, at least not on Market St. When I saw that track, it was already too late. I fell off at about 10 feet away from the bus's front wheel. Fortunately, the bus had not moved yet. It was pretty dangerous when I look back it now. For those who know me enough, they know I often watch video clips like \"top 100 car crashes\", \"Fetal car accidents\" or something like that on Youtube, which is kind of weird. I don't know why I like it but I know it reminds me that I need to be always cautious on the road because there are crazy drivers anywhere. They are super dangerous! I loving biking, either bicycle or motorcycle. I'm kind of mad at myself now as not ever having my own motorcycle even although I had a M1 license 6 years ago. I never got any supports when I said \"I'm gonna to buy a bike.\" I understand it not safe and it seems immature in some sense, but should I just be immature at that age? I would not regret even if I fell off from my own bike if I had one, but I kind of regret I have never had one. Gotta fix my bike soon, meh...","tags":"Life","loc":"http://www.pjthepooh.com/fall-off-the-bike.html","title":"Fall Off the Bike"},{"url":"http://www.pjthepooh.com/python_visualization.html","text":"Intuition Recently my main analysis tool was switched to python, a language that I have not touched for a year. It has never been my first choice for most of data analysis tasks simply because I could not find a GUI that is interactive enough to prototype my ideas fast; however while working on development, python is still on my top list. As a data scientist I hate the feeling that I become pretty clumsy to visualize data using python. While ggplot is still my favourite visualization tool, I want to ramp up a bit on some plotting tools available in python. To be better at using visualization tool, nothing but just practice. As a user of these tools I don't see there are many technical skills invloved; however understanding the workflow and layer structure of the tool is the key. By writting this post, I wish I can get more familiar with the syntax and hope this could help others as well. Below are some popular visualization APIs availabe in python, but I'm going to only use the first 3: matplotlib pandas seaborn boekh ggplot plotly In [1]: import pandas as pd import numpy as np import datetime import random import calendar import matplotlib.pyplot as plt % matplotlib inline import seaborn as sns from scipy import stats Generate a dummy dataset The dataset contains two time series values in daily level. In [2]: def get_monday ( date , format = '%Y-%m- %d ' ): ts = datetime . datetime . strptime ( date , format ) mon_ts = ts - datetime . timedelta ( days = int ( ts . strftime ( '%w' )) - 1 ) return mon_ts . strftime ( format ) In [3]: n = 100 np . random . seed ( 111 ) ts = pd . date_range ( pd . datetime . today (), periods = n ) day = ts . strftime ( '%Y-%m- %d ' ) week = [ get_monday ( d ) for d in day ] wday = ts . strftime ( '%A' ) category = [ random . choice ([ 'A' , 'B' , 'C' ]) for x in range ( n )] value1 = [ 10 + 2 * x + float ( np . random . normal ( 0 , n ** 0.5 , 1 )) for x in range ( n )] value2 = [ 10 + 1.5 * x + float ( np . random . normal ( 0 , n ** 0.7 , 1 )) for x in range ( n )] df = pd . DataFrame ({ \"ts\" : ts , \"day\" : day , \"week\" : week , \"wday\" : wday , \"category\" : category , \"value1\" : value1 , \"value2\" : value2 , }) Pandas' build-in plotting Pandas DataFrame and Series has .plot namespace and there are many ployt types available such as hist, line, scatter , barchart and etc. This is definitely provides a quickest way to vizulize the data, epecially when exploring a DataFrame object. Below are couple simple examples to demo its syntax. Please review Pandas visualization documentation for more detailed explanation. In [4]: fig , ax = plt . subplots ( nrows = 1 , ncols = 2 , figsize = [ 16 , 5 ]) df . plot . line ( ax = ax [ 0 ], x = 'day' , y = [ 'value1' , 'value2' ]) df . plot . scatter ( ax = ax [ 1 ], x = 'value1' , y = 'value2' ) plt . tight_layout () In [5]: dfg = df . groupby ([ 'wday' , 'category' ])[ 'value1' ] . agg ([ np . mean , np . std , np . count_nonzero ]) dfg [ 'mean_std' ] = dfg [ 'std' ] / np . sqrt ( dfg [ 'count_nonzero' ] ) fig , ax = plt . subplots ( figsize = [ 16 , 5 ]) _ = dfg . unstack () . plot ( ax = ax , y = 'mean' , kind = 'bar' , yerr = 'mean_std' , ylim = ( 0 , 250 )) _ = ax . set_xticklabels ( sorted ( df . wday . unique ()), rotation = 0 ) Matplotlib Many python visualization APIs are built on top of Matplotlib, so being good at it is a must. As it is the base of other tools, it provides the most flexible tunning options. Most of the elements in a plot are tunable if you want, making it very powerful for customized plots. Please review Matplotlib documentation for more detailed explanation. For example, we can make a similar plot as above one using a longer way but with more user customization using matplotlib. The data has two key columns (multi-indexed). The way below is to make x-axis as one of the key column, and loop through each unique value of the other key column. While looping, we can customize each group of bars the way we want. In [6]: def gen_ax_bar ( ax , data , group , metric , y_col , std_col = None , width = 0.25 , color = [ 'c' , 'g' , 'y' ], xlab = '' , ylab = '' , title = '' , xticklabel = '' ): group_values = data [ group ] . unique () metric_values = data [ metric ] . unique () axes = [] for i in range ( len ( metric_values )): d = data . loc [ lambda x : x [ metric ] == metric_values [ i ], :] width = width ind = np . arange ( d . shape [ 0 ]) + width * ( i ) y = d [ y_col ] if std_col is not None : std = list ( d [ std_col ]) axes . append ( ax . bar ( ind , y , width , color = color [ i ], yerr = std , error_kw = { 'ecolor' : 'black' , 'linewidth' : 1 })) ax . legend ( axes , metric_values ) ax . set ( xticks = ind , xticklabels = group_values , ylabel = ylab , xlabel = xlab , title = title ) return ax In [7]: df_plt = dfg . reset_index () fig , ax = plt . subplots ( figsize = [ 12 , 6 ]) _ = gen_ax_bar ( ax , df_plt , 'wday' , 'category' , 'mean' , 'mean_std' , color = [ '#00C8F8' , '#FDF200' , '#FFA200' ], xlab = 'Day of week' , ylab = 'Mean of value_1' , title = 'value_1 by Day of week and Category' ) _ = ax . set_ylim ([ 0 , 250 ]) Seaborn Seaborn is highly integrated with matplotlib and pandas' build-in plotting and it is pretty flexible and powerful. You can make an attractive plot with just a few lines of code. It also provides many built-in style options. Please review Seaborn documentation for more detailed explanation. One thing I don't like it is that its plotting functions seem to do too many things, i.e. one function does more than one thing. This somehow makes it hard to be customized. Here is an example that make a simliar bar chart with only couple line of code. Notes that the default will add bootstrapped error bands (good or bad?), which is not the same as the standard error used above. In [8]: fig , ax = plt . subplots ( figsize = [ 16 , 5 ]) _ = sns . barplot ( data = df , x = 'wday' , y = 'value1' , hue = 'category' , n_boot = 1000 , order = sorted ( df . wday . unique ()), hue_order = [ 'A' , 'B' , 'C' ]) _ = ax . set_ylim ([ 0 , 250 ]) More examples to show how easy to make a look-like fancy plot using Seaborn. In [9]: sns . set ( style = \"ticks\" ) fig = sns . JointGrid ( data = df , x = 'value1' , y = 'value2' , size = 5 ) _ = fig . plot_joint ( sns . regplot , color = 'g' ) _ = fig . plot_marginals ( sns . distplot , color = 'g' , bins = 15 ) _ = fig . annotate ( lambda r , p : stats . pearsonr ( r , p )[ 0 ] ** 2 , fontsize = 14 , template = '{stat}: {val:.2f}' , stat = '$R&#94;2$' , loc = 'upper left' ) In [10]: fig = sns . FacetGrid ( df , col = 'wday' , hue = 'wday' , col_wrap = 4 ) _ = fig . map ( sns . regplot , 'value1' , 'value2' , ci = None , order = 1 ) In [11]: fig = sns . FacetGrid ( df [ df . week > min ( df . week )][ 0 : min ( n , 4 * 7 )], row = 'week' , aspect = 5 , margin_titles = True ) _ = fig . map ( sns . kdeplot , 'value1' , shade = True , color = 'y' ) _ = fig . map ( sns . kdeplot , 'value2' , shade = True , color = 'c' ) _ = fig . set_xlabels ( 'value' ) _ = fig . add_legend () for ax in fig . axes . flat : ax . yaxis . set_visible ( False ) fig . fig . subplots_adjust ( hspace = 0.1 ) sns . despine ( left = True ) In [12]: iris = sns . load_dataset ( \"iris\" ) In [13]: def hexbin ( x , y , color , ** kwargs ): cmap = sns . light_palette ( color , as_cmap = True ) plt . hexbin ( x , y , gridsize = 15 , cmap = cmap , ** kwargs ) fig = sns . FacetGrid ( data = iris , hue = 'species' , col = 'species' ) _ = fig . map ( hexbin , 'sepal_length' , 'sepal_width' , extent = [ 3 , 9 , 1 , 5 ]) In [14]: fig = sns . PairGrid ( iris , hue = 'species' ) _ = fig . map_diag ( plt . hist ) _ = fig . map_offdiag ( plt . scatter ) _ = fig . map_lower ( sns . kdeplot , cmap = \"Blues_d\" , alpha = 0.25 ) _ = fig . add_legend () In practice When and where do we need data visualization in building model? Data exploration This is in the early stage when we don't know much about the data. There some fundemental rule of how to pick an appropriate type of plot, e.g. numeric (scatter, line, boxplot, histogram and etc), and categorical (barplot, pie and etc). Model selection In this stage we usually plot values of different paremeters or model performance measurement values by iteration or something like that. Model performance To plot things like feature importance, model performance measurement of the final model. Conclusion and findings To make attractive plots of whatever that can catch audience's attention. Would be worth to spend more time on customization since these plots will be probably on the final output or presentation. Here is a quick example, using GBM to classify survival status on titanic data. In [15]: from sklearn.ensemble import GradientBoostingClassifier from sklearn.grid_search import GridSearchCV In [16]: def gen_param_scores ( CV_fitted ): est = CV_fitted scores = est . grid_scores_ rows = [] params = sorted ( scores [ 0 ] . parameters ) for row in scores : mean = row . mean_validation_score std = row . cv_validation_scores . std () rows . append ([ mean , std ] + [ row . parameters [ k ] for k in params ]) scores = pd . DataFrame ( rows , columns = [ 'score_mean' , 'score_std' ] + params ) return scores In [17]: titanic = sns . load_dataset ( 'titanic' ) Data Exploration In [18]: titanic_g = titanic . groupby ([ 'alive' ]) fig , ax = plt . subplots ( 1 , 2 , figsize = [ 14 , 6 ]) for name , group in titanic_g : ax [ 0 ] . plot ( group . age , group . fare , marker = 'o' , linestyle = '' , ms = 12 , label = name ) _ = ax [ 0 ] . legend ( numpoints = 1 , loc = 'upper right' ) _ = ax [ 0 ] . set ( xlabel = 'Age' , ylabel = 'Fare' , ylim = [ 0 , 150 ], title = 'Survival Status by Age and Fare' ) _ = sns . barplot ( data = titanic , x = 'class' , y = 'survived' , hue = 'sex' , ax = ax [ 1 ], ci = None ) _ = ax [ 1 ] . set ( xlabel = 'Class' , ylabel = 'Survival %' , title = 'Survival Status by Sex and Class' ) Model / parameters selection In [19]: clf = GradientBoostingClassifier () param_grid = dict ( max_depth = [ 1 , 2 , 5 , 10 , 15 , 20 ], min_samples_split = [ 2 , 5 , 10 ], min_samples_leaf = [ 2 , 3 , 4 , 5 ], ) est = GridSearchCV ( clf , param_grid = param_grid , n_jobs = 4 ) y = titanic [ 'survived' ] X = titanic . drop ([ 'survived' , 'who' , 'alive' ], axis = 1 ) X = pd . get_dummies ( X ) . fillna ( value = X . median ()) tmp = est . fit ( X , y ) In [20]: score_table = gen_param_scores ( est ) params = score_table . columns [ 2 :] fig = sns . factorplot ( x = params [ 0 ], y = 'score_mean' , data = score_table , col = params [ 1 ], hue = params [ 2 ], col_wrap = 2 ) In [21]: max_score = score_table . sort_values ( 'score_mean' , ascending = False ) . head ( 1 ) params_selected = max_score [ params ] print max_score score_mean score_std max_depth min_samples_leaf min_samples_split 22 0.821549 0.013746 2 5 5 In [22]: clf = GradientBoostingClassifier ( max_depth = int ( params_selected [ 'max_depth' ]), min_samples_leaf = int ( params_selected [ 'min_samples_leaf' ]), min_samples_split = int ( params_selected [ 'min_samples_split' ]) ) Model performance In [23]: from sklearn.cross_validation import train_test_split from sklearn.metrics import roc_curve , precision_recall_curve , auc def plot_model_perf_curve ( x , y , auc , xlab = '' , ylab = '' , title = '' , legend_loc = 'lower right' , diag_line = True ): plt . figure () plt . plot ( x , y , label = 'Area Under the Curve (area = %0.2f )' % auc ) plt . xlim ([ 0.0 , 1.0 ]) plt . ylim ([ 0.0 , 1.05 ]) plt . xlabel ( xlab ) plt . ylabel ( ylab ) plt . title ( title ) plt . legend ( loc = legend_loc ) if diag_line : plt . plot ([ 0 , 1 ], [ 0 , 1 ], 'k--' ) return plt . show () In [24]: test_pct = 0.3 random_state = np . random . RandomState ( 1 ) X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = test_pct , random_state = random_state ) _ = clf . fit ( X_train , y_train ) y_score = clf . decision_function ( X_test ) In [25]: fpr , tpr , _ = roc_curve ( y_test , y_score ) roc_auc = auc ( fpr , tpr ) plot_model_perf_curve ( fpr , tpr , auc = roc_auc , xlab = 'False Positive Rate' , ylab = 'True Positive Rate' , title = 'Receiver operating characteristic (ROC) Curve' ) In [26]: precision , recall , _ = precision_recall_curve ( y_test , y_score ) pr_auc = auc ( recall , precision ) plot_model_perf_curve ( recall , precision , auc = pr_auc , diag_line = False , xlab = 'Recall' , ylab = 'Precision' , title = 'Precision-Recall Curve' ) In [27]: def plot_feature_importances ( values , labels , xlab = 'Relative Importance' , ylab = '' , title = 'Feature Importance' ): relative_importance = values / max ( values ) * 100 sorted_idx = np . argsort ( relative_importance ) pos = np . arange ( sorted_idx . shape [ 0 ]) + 0.5 plt . subplot ( 1 , 2 , 2 ) plt . barh ( pos , relative_importance [ sorted_idx ], align = 'center' ) plt . yticks ( pos , [ labels [ i ] for i in sorted_idx ]) plt . xlabel ( xlab ) plt . ylabel ( ylab ) plt . title ( title ) return plt . show () In [28]: plt . subplots ( figsize = [ 16 , 5 ]) plot_feature_importances ( clf . feature_importances_ , X . columns . values ) Conlusion I only showed pandas' built-in, matplotlib and seaborn here and others should be similar. I still feel like they are somehow mixed together without a very standardized syntax. Here is probably my temporary rule on how to pick the right tool until I have a better understanding. For a quick view, use panda's built-in first if it is intuitive enough. If not, spend some time on matplotlib. If the data is in a good shape and I can think of a type and style that seaborn has, go for seaborn. I will try to rely more on matplotlib going forward because I want to have a full control of each element. Anyway there is no rule of thumb. It is just about pracitce.","tags":"Data Science","loc":"http://www.pjthepooh.com/python_visualization.html","title":"Python Visualization"},{"url":"http://www.pjthepooh.com/welcome-to-my-new-blog.html","text":"What is PJ the Pooh? This is my nickname created by my girlfriend. This is just my personal page to record my life and share my passion. It is not a pure tech blog. My old habit Time flies. Last time I blogged was when I was in middle school, which is about a decade ago, when Weibo, Facebook and Twitter did not even exist. I was blogging pretty consistently because it helped calm me down as I was in my teenage rebellion period. Of course, the content was mostly about my little things in life and nothing worthy to read if you didn't know me. More importantly, it reminded me of my old stories reading it years later. All the images and feelings would emerge whenever I read it again. My life has changed since my family and I moved to U.S, and then I stopped this habit. Why would I blog again? Why not? Besides the main reason of simply wanting a space for myself, I have the following reasons why I want to do this. My own space - I have full control of this space, from the server to my words. Stays in touch - Mainly to stay in touch with family and friends who I can't see very often. Gain knowledge - Updating blog regularly is a perfect way to stay tuned with the events in my fields. Share knowledge - It may help others who search for similar topics. Store my stuff - It would be convenient to find my own stuff in one place. To practice my writing - I am a terrible writer so here I come. To connect with like-minded people - Hopefully! Anyway, welcome and please keep in touch!","tags":"Life","loc":"http://www.pjthepooh.com/welcome-to-my-new-blog.html","title":"Welcome to My New Blog"}]}