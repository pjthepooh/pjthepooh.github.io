{"pages":[{"url":"http://www.pjthepooh.com/pages/../about.html","text":"My name is PJ, born and raised in Guangzhou, China . I'm currently a data scientist in Silicon Valley , immersing myself in a world of science and new technology. I truly believe that most great decisions are driven by data. I'm willing to learn as much as I can to become a full-stack data scientist. I love sports such as swimming, basketball, snowboarding and bicycling. To be a better version of myself, I love to travel! My another big interest is teaching. I believe that sharing knowledge takes a key role in human society. As a first-generation immigrant in U.S., I would love to contribute my little effort to help the Chinese immigrant community. I speak Cantonese, Mandarin and English.","tags":"Life","loc":"http://www.pjthepooh.com/pages/../about.html","title":"About"},{"url":"http://www.pjthepooh.com/japan-trip.html","text":"Recently several of my friends asked me for tips on Japan because they have planed or are going to plan their first Japan trip, like I did half a year ago. My girlfriend outlined our trip previously so I decided to share it. Introduction Late March of 2016, we went to Japan for the first time. PJ had always wanted to visit Japan, and was even tempted to study the language. We were able to make one of these wishes come true. We stayed in Japan for about 2 weeks, and visited 7 cities. Before the trip we did a lot of research and compiled a detailed plan. This was the first time we have traveled together to a country where neither of us spoke the language. After this unforgettable trip, we decided to share our experience and tips we have collected throughout our trip in this foreign country. Cherry Blossoms Coincidentally, we planned our trip during Spring Break, just in time to see the amazing cherry blossoms bloom in Japan. This iconic flower only blooms for a short period of time, that varies between a 1-2 months period (usually late March to early April) across Japan. If you would like to see the beautiful sight of roads lined with cherry blossoms, be sure to do your homework and check approximately when these pretty flowers will bloom in the area you are visiting. The exact dates varies every year, but the overall pattern is relatively correct. Blooming was later than expected this year, but we got to see the breathtaking sight of cherry blossoms on our last day in Japan, in Ueno Park in Tokyo. The image below shows the forecast for 2016 (as of April 27, 2016); follow the link in the image header to obtain the updated forecast. Season Cherry Blooming Forecast (Date of First Bloom) Osaka Our first stop was Osaka. A must-see sight in Osaka is the Osaka Castle . At this well-known castle, we were able to learn about the history behind the castle and see a slight glimpse into the historical culture of Japan. The castle is surrounded by a moat and the Osaka Castle Park that is home to many cherry blossom trees. The observation deck inside the castle is a great place to see the park. Dotonbori is the place to go for the Times Square-esque light ads. This is where the Glico running man advertisement is. Try to go at night to see the light ads against the dark sky, and walk amongst the crowd of people to either eat lots of great Japanese food or shop around the night market. Kyoto From Osaka, we rode the Shinkansen for 0.5 - 1 hour and arrived at Kyoto Station. Walk out of the station, and you will be greeted by the Kyoto Tower . Kyoto Tower is the tallest building in Kyoto. We suggest you view the tower at night, to see the red tower contrast against the night sky. Kyoto, being the old capitol of Japan, is filled with shrines and temples. Many tourist would visit Kinkakuji Golden Temple. However, we only had one day in Kyoto and the Golden Temple was too far north. In the end, we decided to go to Fushimi Inari-taisha Shrine . This shrine has pathways lined with thousands of tori gates that loops around the back mountain. Hakone The next stop in our adventure was Hakone, to see the famous Fuji Mountain. Deviating from our Airbnb living accommodations, we decided to stay at the Hotel Green Plaza Hakone . This hotel has an onsen with an amazing view of Fuji Mountain. The first day we arrived, the fog was too thick for us to see anything. Fortunately, the next morning the fog disappeared, allowing us to see the magnificent view. The view made the hotel price and the transportation hassle (subway -> train -> bus -> gondola) worth it. Traditional Japanese onsens are separated by men and women baths, and require all bathers to be nude. There are proper steps to using onsens, and specific rules. Be sure to review onsen etiquette before going to one. Tokyo Our fourth destination, was the busy city of Tokyo. Tokyo is a anime and manga lover's dream. Strolling around Akihabara, our childhood self was jumping with joy. The streets were lined with shops selling figurines and items from Slam Dunk to Dragonball Z to Sailor Moon, and much much more. Tokyo Station and Shibuya allowed us to see the bustling culture of Japan. A must-see building in Tokyo is the Tokyo Tower. The observation deck of Tokyo Tower provides you with a 360° view of the city. However, we wanted to see the Tokyo Tower from afar. To do this, we walked to the World Trade Center Observatory . We LOVED this location and the view we were able to see. On the day of our 6th year anniversary, we wanted to step away from the business of Tokyo, and took a quick train ride to Odaiba , an artificial island a bridge away from Tokyo. From the Decks Odaiba, we looked back at the busy heart of Tokyo and saw the buildings and the Rainbow Bridge. A short walk down the boardwalk will lead you to Diver City Plaza, home to a giant Gundam figure. At Ueno Park our line of sight was filled with cherry blossoms no matter where we went. We went on a Sunday; the park floors were covered with people picnicking under the trees. The cherry blossoms, the people, the music and decorations made this an remarkable memory. Yokohama An hour Shinkansen train ride away from Tokyo, is the quitter city of Yokohama. Along the pier are many interesting sites, such as the Yokohama Brick Warehouse (a warehouse turned into a shopping center), a shopping mall, and ferris wheel. We also visited the Cup Noodles Museum and learned about the history of Japanese cup noodles. Let your imagination and food-lover side run wide, and DIY customize your own cup noodles. We were on a tight schedule and could not wait through the long lines. Atami - Izu The last stretch of our trip was Atami and Izu Pennizula. During this last stretch, we stepped away from the bustling city life and reconnected with more nature locations. Right outside of the Atami Station is a feet hotspring, open to the public. At Atami, we ate some cheap but amazingly fresh sushi and sashimi. From Atami, we hopped on a train for 1 hour and arrived in Ito. At Ito, we walked along the Jogasaki Coast , feeling the strong winds and hearing the loud waves. Down the coast is the Kadowaki Bridge, where our childish side took over and we started running down the bridge while the winds blew at our faces. After walking along the coast, we rode a local bus to the Izu Granpal Park . Luckily, there was a lightshow at the park, and we had time to spare. There were lights everywhere, including multiple light models of dolphins :) The light tunnel slide was so much fun, and got us hyped up for watching the rest of the lights. Each section of the park had a different theme and had captivating lightshows. Conclusion All in all, this was an extraordinary trip. The sights, the food, the culture, the childhood memories, allowed us to understand this interesting country. With this trip under our belt, we are able to take on more adventure, and tackle more countries where we do not know the language. Until the next journey! Hope you find something useful here.","tags":"Life","loc":"http://www.pjthepooh.com/japan-trip.html","title":"Japan Trip"},{"url":"http://www.pjthepooh.com/ransac_in_r.html","text":"Intuition Random sample consensus (RANSAC) is one of the techniques that estimate model parameters while the data contains outliers. This technique is not very famous in statistical field, but has been widely used in computer vision. But why? This is also something I want to discuss in this post. A year ago I was working on a project written in R, and wanted to use this technique; however, I was not able to find a package or immediate code in R, but in many other languages such as Matlab, Python and etc. RANSAC's idea is intuitive and the implementation is simple, so I decided to code it up, but I had not actually worked on it until now. In [1]: suppressWarnings ( suppressPackageStartupMessages ({ library ( gridExtra ) library ( pryr ) library ( ggplot2 ) })) First, we need to generate a dataset with trend and noise components, where the trend is 'clean' data points with strong signal and the noise is something that will potentially damage the signal. The noise data is uniformly distributed in a square grid as you can see below. 9 datasets are generated with different noise-to-clean ratios. In [2]: gen_dummy_data <- function ( x , y , noise_ratio = 0.5 ) { if ( length ( x ) != length ( y )) { stop ( \"Length of x and y have to be the same.\" ) } n <- length ( x ) trend <- data.frame ( x = x , y = y ) noise <- data.frame ( x = runif ( noise_ratio * n , min ( x ), max ( x )), y = runif ( noise_ratio * n , min ( y ), max ( y ))) df <- rbind ( trend , noise ) df <- df [ sample ( 1 : nrow ( df ), nrow ( df )), , drop = FALSE ] return ( df ) } In [3]: seed = 1110 m = 100 x <- 1 : m y <- x + rnorm ( m , 0 , 3 ) noise_ratio = c ( 0.25 , 0.5 , 1 , 2 , 3 , 4 , 5 , 8 , 10 ) set.seed ( seed ) df_list <- list () do.call ( partial ( grid.arrange , nrow = 3 ), lapply ( noise_ratio , function ( r ) { df <- gen_dummy_data ( x , y , noise_ratio = r ) df_list [[ as.character ( r )]] <<- df ggplot () + geom_point ( data = df , aes ( x , y )) + ggtitle ( sprintf ( \"noise_ratio = %s\" , r )) }) ) As noise ratio increases, it becomes harder to notice the trend. Let's see if RANSAC is able to detect the trend from the noisy background. Code The R code below is my own version of RANSAC which does not change its principle. This is the first draft of the code for prototype. Please review some materials (or at least Wikipedia ) about this technique for more details. Note: MAE is used for goodness of fit of a model The probablity that the algorithm selects only inliers in some iteration is set to be 0.99 to solve for the maximum number of iteration, k. The initial threshold value is set to be the median of absolute errors of a model fitted using all data points. Default noise ratio is 0.7 to derive other parameters if they are not provided. In [4]: #' Random sample consensus (RANSAC) #' #' @param data dataframe with target column and model matrix columns #' @param y_col name of target column #' @param model model function to fit #' @param model_args list, arguments to the model #' @param n the minimum number of data values required to fit the model #' @param k the maximum number of iterations allowed in the algorithm #' @param t a threshold value for determining when a data point fits a model #' @param d the number of close data values required to assert that a model fits well to data #' #' @return a list of output contains best fitted model, inliers, outliers #' RANSAC <- function ( data , y_col , model , model_args , n = 2 , k = NA , t = NA , d = NA , verbose = FALSE ) { default_ratio = 0.7 model_args_all <- c ( model_args , data = list ( data )) model_all <- do.call ( model , args = model_args_all ) err_abs <- abs ( data [, y_col ] - predict ( model_all , newdata = data [, names ( df ) != y_col , drop = FALSE ])) if ( is.na ( t )) { t <- quantile ( err_abs , 0.5 , names = FALSE ) } if ( is.na ( d )) { d <- nrow ( data ) * default_ratio } if ( is.na ( k )) { k <- as.integer ( log ( 1 - 0.99 ) / log ( 1 - default_ratio &#94; n )) } mae_best <- mean ( err_abs ) ind <- 1 : nrow ( data ) inliers <- c () i = 1 if ( verbose ) { progress_pct = round ( quantile ( seq ( 1 , k , length.out = 10 ), probs = seq ( 0.1 , 1 , length.out = 10 )), 0 ) cat ( sprintf ( \"Begin RANSAC algoritm with parameters:\\nn = %s\\nk = %s\\nt = %s \\nd = %s\\n\\n\" , n , k , t , d )) } while ( i <= k ) { if ( verbose && ( i %in% progress_pct ) ) { cat ( sprintf ( \"Completion percentage: %s\\n\" , names ( progress_pct )[ which ( i == progress_pct )])) } inliers_case <- sample ( ind , n ) model_args_case <- c ( model_args , data = list ( data [ inliers_case , , drop = FALSE ])) model_case <- do.call ( model , model_args_case ) predict_args_case = list ( object = model_case , newdata = data [ - inliers_case , , drop = FALSE ] ) yhat <- do.call ( predict , predict_args_case ) y <- data [ - inliers_case , y_col ] res <- abs ( yhat - y ) inliers_case <- c ( inliers_case , names ( res )[ which ( res < t )]) if ( length ( inliers_case ) > d ) { data_inliers <- data [ inliers_case , , drop = FALSE ] model_inliers_args <- c ( model_args , data = list ( data_inliers )) model_inliers <- do.call ( model , model_inliers_args ) predict_inliers_args_case = list ( object = model_inliers , newdata = data_inliers ) mae_inliers <- mean ( abs ( data_inliers [, y_col ] - do.call ( predict , predict_inliers_args_case ))) if ( mae_inliers < mae_best ) { model_best <- model_inliers mae_best <- mae_inliers inliers <- inliers_case } } i = i + 1 } if ( ! exists ( \"model_best\" )) { warning ( \"Final model could not be found, using all data to fit the model. \" ) model_best <- model_all } model_best [ \"call\" ] <- sprintf ( \"Model with formula: %s\" , model_args [[ \"formula\" ]]) inliers_df = data [ sort ( inliers ), , drop = FALSE ] outliers_df = data [ setdiff ( ind , sort ( inliers )), , drop = FALSE ] if ( verbose ) { cat ( sprintf ( \"%s (%.1f%%) inliers out of %s total data points have been used. \\n\" , length ( inliers ), length ( inliers ) / nrow ( data ) * 100 , nrow ( data ))) } return ( list ( model = model_best , inliers = inliers_df , outliers = outliers_df )) } Next we just loop through different scenarios. For convenience, I hard coded n=5 and k=500 here, which are supposed to be further optimized. In [5]: plot_list <- list () for ( r in noise_ratio ) { args_list = list ( data = df_list [[ as.character ( r )]], y_col = \"y\" , model = lm , model_args = list ( formula = \"y ~ x\" ), n = 5 , k = 500 , t = NA , d = m / ( 1 + r ), verbose = FALSE ) set.seed ( seed ) fit <- RANSAC ( data = args_list [[ \"data\" ]], y_col = args_list [[ \"y_col\" ]], model = args_list [[ \"model\" ]], model_args = args_list [[ \"model_args\" ]], n = args_list [[ \"n\" ]], k = args_list [[ \"k\" ]], t = args_list [[ \"t\" ]], d = args_list [[ \"d\" ]], verbose = args_list [[ \"verbose\" ]] ) p <- ggplot () + geom_point ( data = args_list [[ \"data\" ]], aes ( x = x , y = y ), size = 0.6 ) + stat_smooth ( data = args_list [[ \"data\" ]], aes ( x = x , y = y ), method = \"lm\" , alpha = 0 , color = \"blue\" , size = 1.2 ) + geom_point ( data = fit [[ \"inliers\" ]], aes ( x , y ), color = \"red\" , size = 0.6 ) + stat_smooth ( data = fit [[ \"inliers\" ]], aes ( x = x , y = y ), method = \"lm\" , alpha = 0 , color = \"green\" , size = 1.2 ) + ggtitle ( sprintf ( \"noise_ratio = %s\" , r )) plot_list [[ as.character ( r )]] <- p } In [6]: do.call ( partial ( grid.arrange , nrow = 3 ), lapply ( plot_list , function ( x ) { x })) The blue lines are regular OLS regressions using all data points, so of course they do not fit the trend well. The red points are the inliers selected by the algorithm, and the green lines are best fits. As you can see, RANSAC is able to detect the trend even with high noise rate. It only fails on the last one, where noise-to-clean ratio is 10:1. If you further optimize the parameters, the algorithm may not fail. Discussion Going back to the question: why is RANSAC not widely used in statistics field? There are strong reasons why it is being used in computer vision. There is a concept of noise in CV, where image noise should not give any information to an image. On ther other hand, RANSAC discards a large portion of data, making no attempt to accommodate the outliers. In simple word, it treats the detected outliers that genuinely don't belong to the sample, which is somehow against the philosophy of statistics. Not many statistical approaches attempt to do so. Also, there are many other well-performing robust estimation techniques that better obey statistical philosophy. Ending RANSAC is a quick, simple, intuitive approach for many statistical analyses such as anomaly detection, forecasting, pattern recognition and etc. As people who love statistics and data, we should put it in our tool set. Hope this is helpful and please let me know of any questions regarding this post.","tags":"Data Science","loc":"http://www.pjthepooh.com/ransac_in_r.html","title":"RANSAC in R"},{"url":"http://www.pjthepooh.com/how-to-make-a-blog-with-pelican.html","text":"Intuition There are many benefits of creating a personal website or blog, though it takes time to maintain. Once you decide to make one, finding a framework that you feel comfortable with is important. This website is generated by Pelican , a static site generator powered by python. In this post, I will demonstrate step by step on how to make a blog using Pelican. There are other nice frameworks out there, e.g. Jekyll (ruby), Hexo (javascript) and Nikola (python). These frameworks work well on Windows, OSX, and Linux. For those who are interested in Hexo, please check out my friend's post about Hexo with a step-by-step guide. Ok. Let's get started with Pelican. Step-by-Step Guide Prerequisites I assume you are already comfortable working with python and git . Although it is optional, I recommend the project is built under a virtual environment . Additionally to python and git , which I assume you already installed, you need to install pelican , markdown (as I'm going to use markdown to write contents). I'm going to work on the directory ~/pelican_demo/ for this demo: mkdir ~/pelican_demo cd ~/pelican_demo We next generate a virtual environment and then build necessary libraries within it. virtualenv env source env/bin/activate pip install pelican markdown You may need to install other tools along the way, but let's keep it basic first. Once above packages are installed successfully, we can move to the next step. Quick Start Pelican provides a quick start function, pelican-quickstart , that generates a framework skeleton. I suggest not to create the project in the same directory as where the virtual environment is in; instead, create another directory, e.g. src , for example. mkdir src cd src Once you run the command pelican-quickstart , there is a series of questions popped up, and just answer it one by one. For URL prefix, you can choose n for now. You can leave most of the answers as default (the capitalized one). $ pelican-quickstart Welcome to pelican-quickstart v3.6.3. This script will help you create a new Pelican-based website. Please answer the following questions so this script can generate the files needed by Pelican. > Where do you want to create your new web site? [ . ] > What will be the title of this web site? Pelican Demo > Who will be the author of this web site? PJ > What will be the default language of this web site? [ en ] > Do you want to specify a URL prefix? e.g., http://example.com ( Y/n ) n > Do you want to enable article pagination? ( Y/n ) > How many articles per page do you want? [ 10 ] 8 > What is your time zone? [ Europe/Paris ] America/Los_Angeles > Do you want to generate a Fabfile/Makefile to automate generation and publishing? ( Y/n ) > Do you want an auto-reload & simpleHTTP script to assist with theme and site development? ( Y/n ) > Do you want to upload your website using FTP? ( y/N ) > Do you want to upload your website using SSH? ( y/N ) > Do you want to upload your website using Dropbox? ( y/N ) > Do you want to upload your website using S3? ( y/N ) > Do you want to upload your website using Rackspace Cloud Files? ( y/N ) > Do you want to upload your website using GitHub Pages? ( y/N ) Done. Your new project is available at /Users/pjhuang/pelican_demo/src Now you should have the following folder structure in your src directory. $ tree . ├── Makefile ├── content ├── develop_server.sh ├── fabfile.py ├── output ├── pelicanconf.py └── publishconf.py 2 directories, 5 files Here I will briefly explain these files and folders. Makefile , a program building tool (no need to touch it) content , the directory that will store all your content, e.g. posts, images, pages and etc. Most of your writing will be stored here. develop_server.sh a shell script that boosts your develop server. (again, no need to touch it) fabfile.py You can define your own functions in this file to make the deployment process easier by using fabric. For more details please review Fabric doc . The usage of fab is optional. output , a directory that contains all your outputs that will be shown in the website, e.g. HTMLs, CSSs, images and etc. The website will only consume the files inside this folder. pelicanconf.py a configuration file. You will customize most of your settings in this file. As there are many things you can change, please spend some time to review the setting doc . publishconf.py a configuration file that will be used when publishing. By default, this script will import pelicanconf.py , so you can image this file contains some extra settings when the website goes online. Writing Content Posts and pages are written in Markdown (.md), reStructuredText (.rst) or HTML (.html). I personally prefer Markdown. Pelican will take your plain text file along with the HTML, CSS and whatever necessary files in the some other locations such as your theme directory (will be discussed later) to compile into HTMLs, outputting files into output directory. Let's write a dummy post. New Post To be more organized we can create a subdirectory to store the post. For example, I create a folder named 2016 to store all the posts written in the year 2016. Open your favourite text editor and type in the following and save it as a markdown (*.md) file in directory src/content/2016/ mkdir content/2016 vim content/2016/my_first_post.md Title : My First Post Date : 2016 - 09 - 30 23 : 40 Modified : 2016 - 09 - 30 23 : 40 Category : Pelican Tags : pelican , publishing , python Slug : my - first - post Authors : PJ Summary : This is my first post ! The tags above are called metadata. The necessary fields are Title and Date , others are optional but recommended. New Pages Writing a new page is pretty much the same as writing a post. The main difference is your page files need to be within a folder named pages under content . For example, we can create a About page like this. mkdir content/pages vim content/pages/about.md Title: About Date: 2016-09-30 23:45 Modified: 2016-09-30 23:45 Category: Pelican Tags: pelican, about Slug: ../about Authors: PJ <p> My name is PJ, born and raised in <a href= \"https://en.wikipedia.org/wiki/Guangzhou\" > Guangzhou, China </a> . </p> Now you should have a folder structure like this. src ├── Makefile ├── content │ ├── 2016 │ │ └── my_first_post.md │ └── pages │ └── about.md ├── develop_server.sh ├── fabfile.py ├── output ├── pelicanconf.py ├── pelicanconf.pyc └── publishconf.py Make Files Now you may want to take a look of the actual website content and layout. You can do that by using the built-in make functions defined in the Makefiles and develop_server.sh . Although you don't have to use all make functions to maintain the blog, there are several handy ones you may use frequently. make html - (re)generate the web site make clean - remove the generated files make devserver - start local server (port 8000 as default) make stopserver - stop local server make publish - generate using production First, we need to generate website files, i.e. HTML. $ make html pelican /Users/pjhuang/pelican_demo/src/content -o /Users/pjhuang/pelican_demo/src/output -s /Users/pjhuang/pelican_demo/src/pelicanconf.py Done: Processed 1 article, 0 drafts, 1 page and 0 hidden pages in 0.14 seconds. Second, we start the local server so we can view it locally. make devserver Once the server starts, you can open your browser and type in localhost:8000 , and you will see your website. When the local server is running, any changes in content files will be detected and output files will be regenerated automatically. You can refresh the browser to update the website. Before you publish it (see later section), run this: make publish You can stop the server through: make stopserver Themes This is cool right? What even cooler for Pelican is that it provides many beautiful themes from its themes repository . You can view a screenshot of all these themes in this site . By just a couple steps, you can change your themes quickly. For example, if you want to use the theme blue-penguin , below is what you need to do: Clone this theme's repo in your src directory (you can clone it anywhere but you need to change the theme path in pelicanconf.py accordingly). git clone https://github.com/jody-frankowski/blue-penguin.git And then change (add if you don't see it) the variable THEME in pelicanconf.py . That's it! THEME = 'blue-penguin' You can create your own theme using Jinja . There is a great document about theme creation in Pelican. Plugins Another cool thing about Pelican is it provides a lot of plugins that satisfy most of your needs. Their repo usually have well documented instruction, so here I'm not going to show examples. The steps are similar to those of setting up themes. Again, there is a good documentation about plugin creation if you are interested in. Deployment Hosting on Github Pages Github Pages is one of the best places to host a static website. It is free and provides a 300Mb storage space, which is not bad for a static blog. The deployment process is simple. Register a Github account using your customized username, say, statslover. Create a new repository with the name as this format, <your_username>.github.io and set it as public. In my example, the repo name will be statslover.github.io. Push only your output directory to this repo. There are several ways to finish step 3. As only files inside output directory will be used, so one can initiate a git rooted there and push whatever inside to the repo; however, this is not practical at all because the files including your .git/ could be deleted if you run make clean . Option I You may create a folder that will sync up your output/ directory. For example, I generated a directory named github_page/ one level above src , and wrote a bash script that makes it synced with output/ . My git is in github_page/ and every time before pushing to origin, I sync two folders by running the script. This avoids my git repo being deleted or modified. The command can be something like that: rsync -avz --delete --exclude '.git/' output/ ../github_page Just push it to master as you would do in usual after rsync. cd ~/pelican_demo/github_page git init git remote add origin git@github.com:<your_username>/<your_username>.github.io.git git add . git commit -m\"init commit\" git push -u origin master Option II Another way is to use ghp-import . This module will package your output and push to the repo. I don't like this approach as it generates a gh-pages branch that is the site's document root, which means I would have to deal with two branches somehow. Here are the steps: cd ~/pelican_demo/src pip install ghp-import ghp-import output git push git@github.com:<your_username>/<your_username>.github.io.git gh-pages:master There are many other ways. The process is good as long as you think it is smooth. Once this is done, your website will be shown in the URL <your_username>.github.io. Hosting on other servers If you want to use another server to host the website, you can just simply copy (scp) the files from output/ to the appropriate location of your server. More easily, you can just in that location git clone or pull from your Github repo. Although this requires one more step, but you gain the benefits of using version control. For example, my current workflow is like writting content locally >> generate output files >> sync folder >> push to Github >> pull repo from Github to AWS . This is a bit inconvenient but the work is minimal, so I'm fine with it. Ending Now you should be able to make a static website using Pelican. Hope you think this is helpful. If you have any advice or questions, please leave comments to let me know.","tags":"Geek","loc":"http://www.pjthepooh.com/how-to-make-a-blog-with-pelican.html","title":"How to Make a Blog with Pelican"},{"url":"http://www.pjthepooh.com/i-chose-pelican.html","text":"I have been thinking about making a personal website for awhile, but just could not find time to actually work on it. It takes time but I know it is worth it. A month ago, I was struggling on what framework to use and where to host it. Until recently, I learned and tried three different frameworks and finally decided what is the best for me. Static or Dynamic This was the first question I asked myself. I was biased to dynamic because I want my website to connect with a database, though it is not necessary. The process of setting up the whole dynamic website would definitely help me explore a new area in my life. There is a short way and a long way to accomplish it. The short way is to use a web-hosting service such as Wordpress, Blogger and Weebly; while the long is to build it myself using some web development platform. Anyway, I chose static at the end. Below are the reasons. Django One of the main purposes I'm making a website is trying to learn new things through the process, so of course the drag-and-click feature is not something I would look for. At first I started to learn django. After two weeks of researching online materials, I was able to make a decent skelton successfully connecting to a database, but I was exhausted and not satisfied. I think django is an overkill for my website. Although django is great for professional web development, the investment of learning this framework does not give me a quick pay back. The learning process did help me build a good understanding of web development; I gave up though. Wordpress Using a hosting server, users are able to SSH the server but do not have permission to install applications. Plus, it is costly compared to other approaches. I decided to give it a try and installed wordpress in my AWS EC2 micro. That way at least I would have full control and be able to play around with MySQL. Things went pretty well in the first couple days, but my server began to suffer running out of RAM soon after. In the following week, I kept trying many different ways to optimize the configuration of the web, PHP, and database, nothing seemed to work. I finally moved the database to an RDS instance, but my website was still not able to be hosted smoothly, loading very slowly and even timing out. As it was too annoying, I decided to move to a static web. Pelican For static website, there are several popular platforms, e.g. Hexo (javascript), Jekyll (ruby) and Pelican (python). Users only need to know some basic syntax of the language to begin. I picked pelican simply because it is written in python. All of these frameworks have a large community of developers and users, so there are tons of themes and plugins available. The initial setup of Pelican is simple and it will provide a dummy template to begin with; however, I did need to spend some time on customization and settings. With Pelican's simplicity, I can focus on my writing and not being distracted by thinking how to format my post because the theme does all the jobs for me. If I want to edit the theme, it is easy enough too. So far from writing to deploying a new post, I do not need to touch the mouse, which is a pretty smooth process. Ending Last month was all about trial and error. In the end I was able to learn new things. Most importantly I built my website the way I like. Of course there are some downsides of using Pelican, but I can bear them. I may write up a post about how to build a blog with Pelican later :)","tags":"Geek","loc":"http://www.pjthepooh.com/i-chose-pelican.html","title":"I Chose Pelican"},{"url":"http://www.pjthepooh.com/fall-off-the-bike.html","text":"Today is a bad day. I fell off my bike (bicycle, not motorcycle), and it was my first time falling off. As I did not have any protection, except my helmet of course, and it was in a pretty damn fast speed, so my legs and knees were bruised. Most sadly, my bike crashed on the ground and the handler was deformed. Yup, I bought a new bike a month ago and was about to ride it to work on a daily basis. This morning I was trying to get familiar with the route from home to the office. It was Market St. There was a bus stoping in front of me blocking half of the road. I wanted to accelerate and pass it on the left (yes right? You know you won't stop if you are on a two-wheeled machine). How the hell I could not know there are muni tracks on the middle of Market St (yes right? I must have been living in the city for too long to notice it). I should have noticed it, or I should have never passed buses like that, at least not on Market St. When I saw that track, it was already too late. I fell off at about 10 feet away from the bus's front wheel. Fortunately, the bus had not moved yet. It was pretty dangerous when I look back it now. For those who know me enough, they know I often watch video clips like \"top 100 car crashes\", \"Fetal car accidents\" or something like that on Youtube, which is kind of weird. I don't know why I like it but I know it reminds me that I need to be always cautious on the road because there are crazy drivers anywhere. They are super dangerous! I loving biking, either bicycle or motorcycle. I'm kind of mad at myself now as not ever having my own motorcycle even although I had a M1 license 6 years ago. I never got any supports when I said \"I'm gonna to buy a bike.\" I understand it not safe and it seems immature in some sense, but should I just be immature at that age? I would not regret even if I fell off from my own bike if I had one, but I kind of regret I have never had one. Gotta fix my bike soon, meh...","tags":"Life","loc":"http://www.pjthepooh.com/fall-off-the-bike.html","title":"Fall Off the Bike"},{"url":"http://www.pjthepooh.com/python_visualization.html","text":"Why? Recently my main analysis tool was switched to python, a language that I have not touched for a year. Remember that the first time I used python was because of an NLP project, which python has great packages to deal with. Then with the popularity of pandas at that time, I started implementing ML models using python; however it would not be my first choice for most of the analyses simply because I could not find a GUI that is interactive enough to prototype my ideas fast. I hate the feeling that when I come to visualization in python I became pretty clumsy. After reading several documents and blogs about python visualization, I started liking it. The tools available in python like Matplotlib, pandas's build-in plotting and seaborn are definitely not any worse than ggplot. I mean it is hard to compare amont these tools and make a conclusion that one is better than other, but it would be nice to tell their strengths and weaknesses and pick the right tool to use quickly when tasks come. I decided to write a post to help me ramp up on this, and hope it helps others. Here are some links to some popular visualization tools: matplotlib pandas seaborn Boekh ggplot plotly In [1]: import pandas as pd import numpy as np import datetime import random import calendar import matplotlib.pyplot as plt % matplotlib inline import seaborn as sns from scipy import stats Generate a dummy dataset The dataset contains two time series values in daily level. In [2]: def get_monday ( date , format = '%Y-%m- %d ' ): ts = datetime . datetime . strptime ( date , format ) mon_ts = ts - datetime . timedelta ( days = int ( ts . strftime ( '%w' )) - 1 ) return mon_ts . strftime ( format ) In [3]: n = 100 np . random . seed ( 123 ) ts = pd . date_range ( pd . datetime . today (), periods = n ) day = ts . strftime ( '%Y-%m- %d ' ) week = [ get_monday ( d ) for d in day ] wday = ts . strftime ( '%A' ) category = [ random . choice ([ 'A' , 'B' , 'C' ]) for x in range ( n )] value1 = [ 10 + 2 * x + float ( np . random . normal ( 0 , n ** 0.5 , 1 )) for x in range ( n )] value2 = [ 10 + 1.5 * x + float ( np . random . normal ( 0 , n ** 0.7 , 1 )) for x in range ( n )] df = pd . DataFrame ({ \"ts\" : ts , \"day\" : day , \"week\" : week , \"wday\" : wday , \"category\" : category , \"value1\" : value1 , \"value2\" : value2 , }) In [4]: df . head ( 3 ) Out[4]: category day ts value1 value2 wday week 0 B 2016-09-18 2016-09-18 15:35:13.687869 -0.856306 26.127685 Sunday 2016-09-19 1 A 2016-09-19 2016-09-19 15:35:13.687869 21.973454 -38.182299 Monday 2016-09-19 2 B 2016-09-20 2016-09-20 15:35:13.687869 16.829785 30.891279 Tuesday 2016-09-19 Pandas' build-in plotting Pandas DataFrame and Series has .plot namespace and there are many ployt types available such as hist, line, scatter , barchart and etc. This is definitely provides a quickest way to vizulize the data, epecially when exploring a DataFrame object. In [5]: fig , ax = plt . subplots ( ncols = 2 , figsize = [ 16 , 5 ]) df . plot . line ( ax = ax [ 0 ], x = 'day' , y = [ 'value1' , 'value2' ]) df . plot . scatter ( ax = ax [ 1 ], x = 'value1' , y = 'value2' ) plt . tight_layout () Seaborn I found seaborn is highly integrated with matplotlib and pandas' build-in plotting and it is pretty flexible and powerful. You can make an attractive plot with less than several line of codes. Here are some simple examples. In [6]: fig , ax = plt . subplots ( figsize = [ 16 , 5 ]) sns . barplot ( data = df , x = 'wday' , y = 'value1' , hue = 'category' , ax = ax ) sns . despine () In [7]: sns . set ( style = \"ticks\" ) fig = sns . JointGrid ( data = df , x = 'value1' , y = 'value2' , size = 5 ) _ = fig . plot_joint ( sns . regplot , color = 'g' ) _ = fig . plot_marginals ( sns . distplot , color = 'g' , bins = 15 ) _ = fig . annotate ( lambda r , p : stats . pearsonr ( r , p )[ 0 ] ** 2 , fontsize = 14 , template = '{stat}: {val:.2f}' , stat = '$R&#94;2$' , loc = 'upper left' ) In [8]: fig = sns . FacetGrid ( df , col = 'wday' , hue = 'wday' , col_wrap = 4 ) _ = fig . map ( sns . regplot , 'value1' , 'value2' , ci = None , order = 1 ) In [9]: fig = sns . FacetGrid ( df [ 0 : min ( n , 4 * 7 )], row = 'week' , aspect = 5 , margin_titles = True ) _ = fig . map ( sns . kdeplot , 'value1' , shade = True , color = 'y' ) _ = fig . map ( sns . kdeplot , 'value2' , shade = True , color = 'c' ) _ = fig . set_xlabels ( 'value' ) _ = fig . add_legend () for ax in fig . axes . flat : ax . yaxis . set_visible ( False ) fig . fig . subplots_adjust ( hspace = 0.1 ) sns . despine ( left = True ) In [10]: iris = sns . load_dataset ( \"iris\" ) In [11]: def hexbin ( x , y , color , ** kwargs ): cmap = sns . light_palette ( color , as_cmap = True ) plt . hexbin ( x , y , gridsize = 15 , cmap = cmap , ** kwargs ) fig = sns . FacetGrid ( data = iris , hue = 'species' , col = 'species' ) _ = fig . map ( hexbin , 'sepal_length' , 'sepal_width' , extent = [ 3 , 9 , 1 , 5 ]) In [12]: fig = sns . PairGrid ( iris , hue = 'species' ) _ = fig . map_diag ( plt . hist ) _ = fig . map_offdiag ( plt . scatter ) _ = fig . map_lower ( sns . kdeplot , cmap = \"Blues_d\" , alpha = 0.25 ) _ = fig . add_legend () In practice Here is a simple example to use seaborn to find a good combination of model parameters In [13]: from sklearn.ensemble import RandomForestClassifier from sklearn.grid_search import GridSearchCV In [14]: titanic = sns . load_dataset ( 'titanic' ) In [15]: titanic . head ( 3 ) Out[15]: survived pclass sex age sibsp parch fare embarked class who adult_male deck embark_town alive alone 0 0 3 male 22.0 1 0 7.2500 S Third man True NaN Southampton no False 1 1 1 female 38.0 1 0 71.2833 C First woman False C Cherbourg yes False 2 1 3 female 26.0 0 0 7.9250 S Third woman False NaN Southampton yes True In [16]: clf = RandomForestClassifier () param_grid = dict ( max_depth = [ 1 , 2 , 5 , 10 , 20 , 30 , 40 , 50 ], min_samples_split = [ 2 , 5 , 10 ], min_samples_leaf = [ 2 , 3 , 4 , 5 ], ) est = GridSearchCV ( clf , param_grid = param_grid , n_jobs = 4 ) In [17]: y = titanic [ 'survived' ] X = titanic . drop ([ 'survived' , 'who' , 'alive' ], axis = 1 ) X = pd . get_dummies ( X ) . fillna ( value = X . median ()) est . fit ( X , y ) Out[17]: GridSearchCV(cv=None, error_score='raise', estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini', max_depth=None, max_features='auto', max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1, oob_score=False, random_state=None, verbose=0, warm_start=False), fit_params={}, iid=True, n_jobs=4, param_grid={'min_samples_split': [2, 5, 10], 'max_depth': [1, 2, 5, 10, 20, 30, 40, 50], 'min_samples_leaf': [2, 3, 4, 5]}, pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0) In [18]: def gen_param_scores ( CV_fitted ): est = CV_fitted scores = est . grid_scores_ rows = [] params = sorted ( scores [ 0 ] . parameters ) for row in scores : mean = row . mean_validation_score std = row . cv_validation_scores . std () rows . append ([ mean , std ] + [ row . parameters [ k ] for k in params ]) scores = pd . DataFrame ( rows , columns = [ 'score_mean' , 'score_std' ] + params ) return scores In [19]: score_table = gen_param_scores ( est ) params = score_table . columns [ 2 :] fig = sns . factorplot ( x = params [ 0 ], y = 'score_mean' , data = score_table , col = params [ 1 ], hue = params [ 2 ], col_wrap = 2 ) In [20]: score_table . sort_values ( 'score_mean' , ascending = False ) . head ( 1 ) Out[20]: score_mean score_std max_depth min_samples_leaf min_samples_split 46 0.828283 0.036055 10 5 5","tags":"Data Science","loc":"http://www.pjthepooh.com/python_visualization.html","title":"Python Visualization"},{"url":"http://www.pjthepooh.com/welcome-to-my-new-blog.html","text":"Welcome to my new blog! Time flies. Last time I wrote blog was when I was in middle school, which is n yeas ago, n > 10, when Weibo, Facebook, Twitter did not even exist, but QQ was super popular in China at that time (not quite sure what the outside world was going on). Most young people wrote status or whatever personal stuff on QQ zone, an online social platform similar to Weibo, Facebook today (hmm, actually not quite, but you can imagine it). I did not write it there; instead I found a more quiet place to express my thoughts, a diary website. I was blogging pretty consistently because I felt writing a diary or blog really calmed me down. More importantly, it reminds me my old stories even after years. Why would I","tags":"Life","loc":"http://www.pjthepooh.com/welcome-to-my-new-blog.html","title":"Welcome to My New Blog"}]}